{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T13:39:53.814035Z",
     "start_time": "2019-04-09T13:39:51.063378Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T13:40:05.314362Z",
     "start_time": "2019-04-09T13:39:53.838960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T13:41:43.383425Z",
     "start_time": "2019-04-09T13:41:43.312539Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "\n",
    "    def __init__(self): # define the structure\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=5, padding=2), # conv+bn+relu+pooling\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=5, padding=2), # conv+bn+relu+pooling\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10) # fully-connected layer\n",
    "        \n",
    "    def forward(self, x): # define forward(inference) function(do not need backpopagation function)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = nn.CrossEntropyLoss() # contain softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:43:44.900667Z",
     "start_time": "2019-04-09T13:41:44.230083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 468/468 [02:38<00:00,  2.95it/s]\n",
      "Epoch [1/10], Training Loss: 0.0821, Training Accuracy: 98.3357%: 100%|█| 468/468 [01:23<00:00,  5.61it/s]\n",
      "Epoch [1/10], Testing Loss: 0.0821, Testing Accuracy: 98.2572%: 100%|█| 78/78 [00:15<00:00,  5.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:08<00:00,  2.49it/s]\n",
      "Epoch [2/10], Training Loss: 0.0375, Training Accuracy: 98.9683%: 100%|█| 468/468 [01:44<00:00,  4.48it/s]\n",
      "Epoch [2/10], Testing Loss: 0.0375, Testing Accuracy: 98.8682%: 100%|█| 78/78 [00:17<00:00,  4.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:45<00:00,  2.08it/s]\n",
      "Epoch [3/10], Training Loss: 0.0574, Training Accuracy: 99.1854%: 100%|█| 468/468 [02:14<00:00,  3.47it/s]\n",
      "Epoch [3/10], Testing Loss: 0.0574, Testing Accuracy: 99.0585%: 100%|█| 78/78 [00:18<00:00,  4.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [04:00<00:00,  1.95it/s]\n",
      "Epoch [4/10], Training Loss: 0.0081, Training Accuracy: 99.3323%: 100%|█| 468/468 [01:45<00:00,  4.42it/s]\n",
      "Epoch [4/10], Testing Loss: 0.0081, Testing Accuracy: 98.9283%: 100%|█| 78/78 [00:17<00:00,  4.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [04:06<00:00,  1.90it/s]\n",
      "Epoch [5/10], Training Loss: 0.0033, Training Accuracy: 99.4491%: 100%|█| 468/468 [03:56<00:00,  1.98it/s]\n",
      "Epoch [5/10], Testing Loss: 0.0033, Testing Accuracy: 99.0885%: 100%|█| 78/78 [00:28<00:00,  2.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [05:40<00:00,  1.38it/s]\n",
      "Epoch [6/10], Training Loss: 0.0217, Training Accuracy: 99.4641%: 100%|█| 468/468 [02:39<00:00,  2.93it/s]\n",
      "Epoch [6/10], Testing Loss: 0.0217, Testing Accuracy: 99.0685%: 100%|█| 78/78 [00:16<00:00,  4.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:42<00:00,  2.10it/s]\n",
      "Epoch [7/10], Training Loss: 0.0062, Training Accuracy: 99.5743%: 100%|█| 468/468 [02:19<00:00,  3.36it/s]\n",
      "Epoch [7/10], Testing Loss: 0.0062, Testing Accuracy: 98.9383%: 100%|█| 78/78 [00:19<00:00,  4.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:36<00:00,  2.16it/s]\n",
      "Epoch [8/10], Training Loss: 0.0052, Training Accuracy: 99.7312%: 100%|█| 468/468 [01:42<00:00,  4.57it/s]\n",
      "Epoch [8/10], Testing Loss: 0.0052, Testing Accuracy: 99.0485%: 100%|█| 78/78 [00:17<00:00,  4.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:40<00:00,  2.12it/s]\n",
      "Epoch [9/10], Training Loss: 0.0004, Training Accuracy: 99.8247%: 100%|█| 468/468 [01:47<00:00,  4.37it/s]\n",
      "Epoch [9/10], Testing Loss: 0.0004, Testing Accuracy: 99.1987%: 100%|█| 78/78 [00:15<00:00,  5.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 468/468 [03:25<00:00,  2.27it/s]\n",
      "Epoch [10/10], Training Loss: 0.0017, Training Accuracy: 99.5827%: 100%|█| 468/468 [01:41<00:00,  4.63it/s]\n",
      "Epoch [10/10], Testing Loss: 0.0017, Testing Accuracy: 98.9583%: 100%|█| 78/78 [00:17<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        # TODO:forward + backward + optimize\n",
    "        optimizer.zero_grad() # zero the stored gradient\n",
    "        outputs = model(images) # forward\n",
    "        loss = criterion(outputs, labels) # calculate loss\n",
    "        loss.backward() # backpopagate loss\n",
    "        optimizer.step() # update weights        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    cnt = 0\n",
    "    tbar = tqdm(train_loader) # a progress bar of training process\n",
    "    for images, labels in tbar:\n",
    "        outputs = model(images)\n",
    "        pre_labels = outputs.max(dim=1)[1]\n",
    "        cnt += (pre_labels == labels).sum()\n",
    "    \n",
    "        acc = float(cnt) / len(train_loader) / BATCH_SIZE\n",
    "        tbar.set_description('Epoch [%d/%d], Training Loss: %.4f, Training Accuracy: %.4f%%' % (epoch+1, NUM_EPOCHS, loss.item(), acc*100))\n",
    "    \n",
    "    cnt = 0\n",
    "    tbar = tqdm(test_loader)\n",
    "    for images, labels in tbar:\n",
    "        outputs = model(images)\n",
    "        pre_labels = outputs.max(dim=1)[1]\n",
    "        cnt += (pre_labels == labels).sum()\n",
    "    \n",
    "        acc = float(cnt) / len(test_loader) / BATCH_SIZE\n",
    "        tbar.set_description('Epoch [%d/%d], Testing Loss: %.4f, Testing Accuracy: %.4f%%' % (epoch+1, NUM_EPOCHS, loss.item(), acc*100))    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
